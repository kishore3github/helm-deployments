# .github/workflows/main.yaml
name: Consolidated Deployment Workflow

on:
  push:
    branches:
      - main
    paths:
      - 'consolidated-deployments.yaml'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Mode of operation'
        required: true
        default: 'sequential'
        type: choice
        options:
          - sequential
          - parallel

jobs:
  read-services:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install yq and jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          sudo snap install yq

      - name: Read deployment.yaml and deploy sequentially
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
            i=0
            yq -o=json '.deployments' consolidated-deployments.yaml | jq -c '.[]' | while read -r serviceGroup; do
              echo "=== Deploying group $i in parallel ==="
              run_ids_file="run_ids_$i.txt"
              > "$run_ids_file"

              # Read all services in the group into an array (including duplicates)
              mapfile -t services < <(echo "$serviceGroup" | jq -c '.[]')

              pids=()
              for svc in "${services[@]}"; do
                name=$(echo "$svc" | jq -r '.name')
                ns=$(echo "$svc" | jq -r '.namespace')
                image=$(echo "$svc" | jq -r '.image')
                flyway=$(echo "$svc" | jq -r '.flyway')

                echo "Triggering $name in namespace $ns with image $image and flyway=$flyway"
                (
                  # Trigger workflow and capture the run id directly from the output
                  run_output=$(gh workflow run reusable_cd.yaml \
                    -f service="$name" \
                    -f image_tag="$image" \
                    -f namespace="$ns" \
                    -f flyway="$flyway" \
                    --json id 2>/dev/null)

                  new_run_id=$(echo "$run_output" | jq -r '.id // empty')

                  # Fallback: If direct capture fails, poll for the latest run for this workflow and inputs
                  if [[ -z "$new_run_id" ]]; then
                    for attempt in {1..12}; do
                      sleep 5
                      # Get the most recent run for this workflow and branch
                      new_run_id=$(gh run list --workflow=reusable_cd.yaml --branch="$GITHUB_REF_NAME" --json databaseId,headBranch,headSha -q '.[0].databaseId')
                      if [[ -n "$new_run_id" ]]; then
                        break
                      fi
                    done
                  fi

                  if [[ -z "$new_run_id" ]]; then
                    echo "❌ Failed to get new run ID for $name"
                    exit 1
                  fi

                  echo "Run ID for $name in $ns: $new_run_id"
                  echo "$new_run_id" >> "$run_ids_file"
                ) &
                pids+=($!)
              done

              # Wait for all background triggers to finish
              for pid in "${pids[@]}"; do
                wait "$pid"
              done

              # Now watch all runs in parallel
              watch_pids=()
              while read -r id; do
                (
                  while true; do
                    status=$(gh run view "$id" --json status -q '.status')
                    conclusion=$(gh run view "$id" --json conclusion -q '.conclusion')
                    echo "Workflow $id status: $status / $conclusion"

                    if [[ "$status" == "completed" ]]; then
                      if [[ "$conclusion" == "success" ]]; then
                        echo "✅ Run $id succeeded"
                        break
                      else
                        echo "❌ Run $id failed or cancelled"
                        exit 1
                      fi
                    fi
                    sleep 10
                  done
                ) &
                watch_pids+=($!)
              done < "$run_ids_file"

              # Wait for all watches to finish
              for pid in "${watch_pids[@]}"; do
                wait "$pid"
              done

              rm -f "$run_ids_file"
              echo "✅ Group $i complete"
              ((i+=1))
            done
